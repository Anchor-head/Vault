# ðŸ¤” What are ensemble methods?
---
Instead of just using one predictive model, why not use a bunch of classifiers and return the majority classification? That's what ensemble methods are.

As it turns out, this can improve the error rate a lot â€” to the extent that the different classifiers are independent and their error is better than random (a committee of clones is quite useless, and a committee of blinds is also useless).

![[Pasted image 20250206175020.png]][[Pasted image 20250206175542.png|Here is the formula for the error rate of an ensemble]] of $N$ independent classifiers with error rate $Îµ$.

# ðŸ‘œ Bagging
---
![[Pasted image 20250207111747.png]]
Bagging (short for "bootstrap aggregation") involves creating multiple training sets out of the original training via bootstrapping (bootstrapping is resampling with replacement). The training sets created this way are called "bootstraps," and are generally the same size as the original training set.
![[Pasted image 20250207111922.png]]


# ðŸš€ Boosting
---
![[Pasted image 20250207112045.png]]
Boosting involves sequentially training a model on different non-random splits of the training data, such that each split overrepresents the misclassifications made by a previous classifier trained on the previous split:

![[Pasted image 20250207112404.png]]![[Pasted image 20250207135110.png]]

The first subsample of the data is a random sample (without replacement) of the training data, while the second model, being adversarially trained on the errors of the first model (outside the first subsample), is specifically designed to classify the data that the first model fails to accurately classify. Finally, the third subsample is just the still-unsampled training data.

Schapire proved that splitting the training data in this way improves performance.


# ðŸŒ² Random Forest
