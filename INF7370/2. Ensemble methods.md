# ðŸ¤” What are ensemble methods?
---
Instead of just using one predictive model, why not use a bunch of classifiers and return the majority classification? That's what ensemble methods are.

As it turns out, this can improve the error rate a lot â€” to the extent that the different classifiers are independent and their error is better than random (a committee of clones is quite useless, and a committee of blinds is also useless).

![[Pasted image 20250206175020.png]][[Pasted image 20250206175542.png|Here is the formula for the error rate of an ensemble]] of $N$ independent classifiers with error rate $Îµ$.

# ðŸ‘œ Bagging
---
![[Pasted image 20250207111747.png]]
Bagging (short for "bootstrap aggregation") involves creating multiple training sets out of the original training via bootstrapping (bootstrapping is resampling with replacement). The training sets created this way are called "bootstraps," and are generally the same size as the original training set.
![[Pasted image 20250207111922.png]]


# ðŸš€ Boosting
---
![[Pasted image 20250207112045.png]]
Boosting involves sequentially training a model on different non-random splits of the training data, such that each split overrepresents the misclassifications made by the previous classifier trained on the previous split.
#### Elementary boosting

![[Pasted image 20250207112404.png]]![[Pasted image 20250207135110.png]]

The first subsample of the data (S1) is a random sample (without replacement) of the training data, and is used to train a classifier h1; the second subsample (S2) is selected to overrepresent errors made by h1 on the rest of the training data; thus, the second classifier, h2, is adversarial to h1. Finally, the third subsample (S3) used to train h3 is just the still-unsampled training data (S3 = S-S1-S2). Schapire proved that splitting the training data in this way improves performance.
#### Probabilistic boosting


# ðŸŒ² Random Forest
