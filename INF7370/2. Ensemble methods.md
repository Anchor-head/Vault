# 🤔 What are ensemble methods?
---
Instead of just using one predictive model, why not use a bunch of classifiers and return the majority classification? That's what ensemble methods are.

As it turns out, this can improve the error rate a lot — to the extent that the different classifiers are independent and their error is better than random (a committee of clones is quite useless, and a committee of blinds is also useless).

![[Pasted image 20250206175020.png]][[Pasted image 20250206175542.png|Here is the formula for the error rate of an ensemble]] of $N$ independent classifiers with error rate $ε$.

# 👜 Bagging
---
![[Pasted image 20250207111747.png]]
Bagging (short for "bootstrap aggregation") involves creating multiple training sets out of the original training via bootstrapping (bootstrapping is resampling with replacement). The training sets created this way are called "bootstraps," and are generally the same size as the original training set.
![[Pasted image 20250207111922.png]]


# 🚀 Boosting
---
![[Pasted image 20250207112045.png]]
Boosting involves sequentially training a classifier on different non-random splits of the training data; the point is to make each split overrepresent the misclassifications made by the classifier trained on the previous split, to continually “train them on their weakness” and create adversarial classifiers that cover each other’s blind spots.
#### Elementary boosting

![[Pasted image 20250207112404.png]]![[Pasted image 20250207135110.png]]
Schapire proved that splitting the training data in this way does indeed improve performance.
#### Probabilistic boosting
>[!warning]  Sampling
>This time, sampling is done WITH replacement, and samples are the size of the original training data.

The idea is to increase the weight of misclassified data every sample.

Initially, all data points in the training set have equal probability of appearing in the first sample, and the weight of misclassified data are increased every sample.

# 🌲 Random Forest
