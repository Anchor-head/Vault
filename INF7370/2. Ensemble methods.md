# ðŸ¤” What are ensemble methods?
---
Instead of just using one predictive model, why not use a bunch of classifiers and return the majority classification? That's what ensemble methods are.

As it turns out, this can improve the error rate a lot â€” to the extent that the different classifiers are independent and their error is better than random (a committee of clones is quite useless, and a committee of blinds is also useless).

![[Pasted image 20250206175020.png]][[Pasted image 20250206175542.png|Here is the formula for the error rate of an ensemble]] of $N$ independent classifiers with error rate $Îµ$.

# ðŸ‘œ Bagging
---
![[Pasted image 20250207111747.png]]
Bagging (short for "bootstrap aggregation") involves creating multiple training sets out of the original training via bootstrapping (bootstrapping is resampling with replacement). The training sets created this way are called "bootstraps," and are generally the same size as the original training set.
![[Pasted image 20250207111922.png]]


# ðŸš€ Boosting
---
![[Pasted image 20250207112045.png]]
Boosting involves sequentially training a model on subsamples of the training data selected in the following way:

![[Pasted image 20250207112404.png]]
The first subsample of the data is random, while the second subsample emphasizes/overrepresents errors made by the model trained on the first subsample. In a sense, the second model is specifically designed to counteract the first model's weaknesses, in an almost adversarial way.
# ðŸŒ² Random Forest
