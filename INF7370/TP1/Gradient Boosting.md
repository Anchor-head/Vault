
Le Gradient Boosting (GBoost) est une méthode d'apprentissage machine qui produit un ensemble d'arbres de régression que l'on additionne pour obtenir une prédiction. Les arbres sont entrainés séquentiellement, où chaque nouvel arbre tâche à prédire les résidus de l'ensemble d'arbres précédents (ou plutôt les pseudo-résidus, c'est-à-dire le gradient négatif de la fonction objectif de l'ensemble, qui revient au vrai résidu si on utilise les fonctions objectifs les plus communes: l'erreure quadratique dans le cas d'une tâche de régression et la log-vraisemblance dans le cas d'une tâche de classification). Par exemple, si le premier arbre n'a qu'une feuille et prédit la variable dépendante $y$ par la moyenne de ses éléments ($\hat{y}_1=\bar{y}$) alors le deuxième arbre a pour objectif de prédire les résidus $r_1$ produits par le premier arbre, $r_1=y_1-\hat{y}_1=y_1-\bar{y}$. L'addition des prédictions du deuxième arbre au premier raffine la prédiction de l'ensemble; $\hat{y}_2=\hat{y}_1+\nu*\hat{r}_1=\bar{y}+\nu*\hat{r}_1$ (où $\nu$ est le *taux d'apprentissage*, un hyperparamètre déterminant combien chaque arbre additionnel doit contribuer à la prédiction finale) est une meilleure prédiction que $\hat{y}_1$ seul. Ensuite, le troisième arbre prédira les résidus $r_2$ produits par l'ensemble à présent, c'est-à-dire $r_2=y_1-(\hat{y}_1+\nu*\hat{r}_1)=y_1-(\bar{y}+\nu*\hat{r}_1)$, et ce troisième arbre sera ajouté à l'ensemble de la même manière pour encore en raffiner la prédiction; $\hat{y}_3=\bar{y}+\nu*\hat{r}_1+\nu*\hat{r}_2$, et ainsi de suite. Certains ensemble peuvent contenir des centaines d'arbres ainsi conçus.

Chaque arbre suivant le premier (celui qui ne fait que regrouper tous les exemples sous une feuille et prédire la moyenne) est un petit arbre de régression qui tente de minimiser l'erreure quadratique (ou l'erreure quadratique Friedman) selon l'algorithme habituel de construction d'arbre.