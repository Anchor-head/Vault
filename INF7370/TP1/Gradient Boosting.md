
Le Gradient Boosting (GBoost) est une méthode d'apprentissage machine qui produit un ensemble d'arbres de régression que l'on additionne pour obtenir une prédiction. Les arbres sont entrainés séquentiellement, où chaque nouvel arbre tâche à prédire les résidus de l'ensemble d'arbres précédents. Par exemple, si le premier arbre n'a qu'une feuille et prédit la variable dépendante $y$ par la moyenne de ses éléments, $\hat{y}_1=\bar{y}_1$, alors le deuxième arbre a pour objectif de prédire les résidus $r_1$ produits par le premier arbre, $r_1=y_1-\hat{y}_1=y_1-\bar{y}_1$. L'addition des prédictions du deuxième arbre au premier raffine la prédiction de l'ensemble; $\hat{y}_2=\hat{y}_1+\nu*\hat{r}_1=\bar{y}_1+\nu*\hat{r}_1$ (où $\nu$ est le *taux d'apprentissage*, un hyperparamètre déterminant combien chaque arbre additionnel doit contribuer à la prédiction finale) est une meilleure prédiction que $\hat{y}_1$ seul. Ensuite, le troisième arbre prédira les résidus $r_2$ produits par l'ensemble à présent, c'est-à-dire $r_2=y_1-(\hat{y}_1+\nu*\hat{r}_1)=y_1-(\bar{y}_1+\nu*\hat{r}_1)$, et ce troisième arbre sera ajouté à l'ensemble de la même manière pour encore en raffiner la prédiction; $\hat{y}_3=\bar{y}_1+\nu*\hat{r}_1+\nu*\hat{r}_2$, et ainsi de suite. Certains ensemble peuvent contenir des centaines d'arbres ainsi conçus.

