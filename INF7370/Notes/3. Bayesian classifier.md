# ü§î What is a [[3. Classification bay√©sienne.pdf|na√Øve Bayesian classifier]]?
---
A Bayesian classifier maximizes the objective function $P(Y|X)$. **However**, with many feature variables $X$, calculating $P(Y|X_1, X_2, ..., X_d)$ becomes intractable.^[https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Probabilistic_model:~:text=The%20problem%20with%20the%20above%20formulation%20is%20that%20if%20the%20number%20of%20features%20n%20is%20large%20or%20if%20a%20feature%20can%20take%20on%20a%20large%20number%20of%20values%2C%20then%20basing%20such%20a%20model%20on%20probability%20tables%20is%20infeasible.%20The%20model%20must%20therefore%20be%20reformulated%20to%20make%20it%20more%20tractable] **So**, we use Baye‚Äôs theorem and one (na√Øve) conditional independence assumption to simplify things.

First, by Baye's Theorem, we can transform $P(Y|X)$ into:

$$P(Y|X)=\frac{P(Y)*P(X|Y)}{P(X)}$$

 Now, $P(X|Y)$ is still hard to calculate^[https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Probabilistic_model:~:text=The%20numerator%20is,conditional%20probability%3A], but if we assume that all features $X$ are conditionally independent given $Y$, we get:
$$\frac{P(Y)*P(X|Y)}{P(X)} = \frac{P(Y)*\prod_{i=1}^{d}P(X_i|Y)}{P(X)}$$
Now, excluding the constant denominator $P(X)$, we get the objective function that the Bayesian classifier seeks to maximize:
$$P(Y)*\prod_{i=1}^{d}P(X_i|Y)$$
>[!warning]- Implications of conditional independence
> - Conditional independence is NOT reciprocal; that is, $P(X,Y|Z) = P(X|Z)*P(Y|Z)$ does *not* imply $P(X,Z|Y) = P(X|Y)*P(Z|Y)$
> - Conditional independence does NOT imply unconditional independence (see figure below)
>
> ![[Pasted image 20250212154905.png|300]]
> The area outlined in red is event $Z$, the area in blue is event $A$, in lavender event $B$, and in purple is $A \cap B$. Notice:
> 1. A and B are conditionally independent given $Z$ or $Z‚Äô$, but not unconditionally independent.

[[3. Classification bay√©sienne.pdf#page=21|Here is an example]] of a na√Øve Bayesian classifier algorithm applied to document classification.
#### Numerical variables

There are two ways of dealing with numerical variables in a Bayesian classifier:
1. Split the numerical variable into intervals, just like for [[1. Decision trees#^numvar|numerical variables in decision trees]].
2. Assume $X|Y \sim N(\mu,\sigma^2)$ and assign $P(X=x|Y) = PDF_{X|Y}(x) = \phi_{\mu,\sigma}(x)$. More explicitly: ![[3. Classification bay√©sienne.pdf#page=13]]