# ðŸ¤” What is a [[3. Classification bayÃ©sienne.pdf|naÃ¯ve Bayesian classifier]]?
---
A Bayesian classifier maximizes the objective function $P(Y|X)$.

However, with many feature variables $X$, calculating $P(Y|X_1, X_2, ..., X_d)$ becomes intractable.
There is one (naÃ¯ve) assumption that allows us to simplify this formula, however.

First, by Baye's Theorem,
$$P(Y|X) = \frac{P(Y)*P(X|Y)}{P(X)}.$$
Now the difficult thing to calculate here is $P(X|Y)$; however, if we assume that all X are conditionally independent given Y, we get
$$\frac{P(Y)*P(X|Y)}{P(X)} = \frac{P(Y)*\prod_{i=1}^{d}P(X_i|Y)}{P(X)}.$$
Now, excluding the constant denominator, we get the objective function that the Bayesian classifier seeks to maximize:
$$P(Y)*\prod_{i=1}^{d}P(X_i|Y).$$
>[!faq] Implications of conditional independence
> - Conditional independence is NOT reciprocal; that is, $P(X,Y|Z) = P(X|Z)*P(Y|Z)$ does *not* imply $P(X,Z|Y) = P(X|Y)*P(Z|Y)$
> - Conditional independence does NOT imply unconditional independence (see figure below)
>
> ![[Pasted image 20250212154905.png|300]]
> The area outlined in red is event $Z$, the area in blue is event $A$, in lavender event $B$, and in purple is $A \cap B$. Notice:
> 1. A and B are conditionally independent given $Z$ or $Zâ€™$, but not unconditionally independent.

[[3. Classification bayÃ©sienne.pdf#page=21|Here is an example]] of a naÃ¯ve Bayesian classifier algorithm applied to document classification.
# ðŸ”¢ Numerical variables

There are two ways of dealing with numerical variables in a Bayesian classifier:
1. Split the numerical variable into intervals, just like we did for [[1. Decision trees#^numvar|numerical variables in decision trees]].
2. Assign $P(X|Y) = N(x,\sigma_x)$