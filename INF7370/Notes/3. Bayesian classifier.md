# ü§î What is a [[3. Classification bay√©sienne.pdf|na√Øve Bayesian classifier]]?
---
A Bayesian classifier maximizes the objective function $P(Y|X)$. **However**, with many feature variables $X$, calculating $P(Y|X_1, X_2, ..., X_d)$ becomes intractable. **So**, we use Baye‚Äôs theorem and one (na√Øve) conditional independence assumption to simplify things.

First, by Baye's Theorem, we can transform $P(Y|X)$ into:

$$P(Y|X)=\frac{P(Y)*P(X|Y)}{P(X)}$$

 Now, $P(X|Y)$ is still hard to calculate, but if we assume that all features $X$ are conditionally independent given $Y$, we get:
$$\frac{P(Y)*P(X|Y)}{P(X)} = \frac{P(Y)*\prod_{i=1}^{d}P(X_i|Y)}{P(X)}$$
Now, excluding the constant denominator $P(X)$, we get the objective function that the Bayesian classifier seeks to maximize:
$$P(Y)*\prod_{i=1}^{d}P(X_i|Y)$$
>[!warning]- Implications of conditional independence
> - Conditional independence is NOT reciprocal; that is, $P(X,Y|Z) = P(X|Z)*P(Y|Z)$ does *not* imply $P(X,Z|Y) = P(X|Y)*P(Z|Y)$
> - Conditional independence does NOT imply unconditional independence (see figure below)
>
> ![[Pasted image 20250212154905.png|300]]
> The area outlined in red is event $Z$, the area in blue is event $A$, in lavender event $B$, and in purple is $A \cap B$. Notice:
> 1. A and B are conditionally independent given $Z$ or $Z‚Äô$, but not unconditionally independent.

[[3. Classification bay√©sienne.pdf#page=21|Here is an example]] of a na√Øve Bayesian classifier algorithm applied to document classification.
#### Numerical variables

There are two ways of dealing with numerical variables in a Bayesian classifier:
1. Split the numerical variable into intervals, just like for [[1. Decision trees#^numvar|numerical variables in decision trees]].
2. Assume $X|Y \sim N(\mu,\sigma^2)$ and assign $P(X=x|Y) = PDF_{X|Y}(x) = \phi_{\mu,\sigma}(x)$. More explicitly: ![[3. Classification bay√©sienne.pdf#page=13]]