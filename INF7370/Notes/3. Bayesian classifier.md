# ğŸ¤” What is a [[3. Classification bayÃ©sienne.pdf|naÃ¯ve Bayesian classifier]]?

A Bayesian classifier maximizes the objective function $P(Y|X)$.

However, with many feature variables $X$, calculating $P(Y|X_1, X_2, ..., X_d)$ becomes intractable.
There is one (naÃ¯ve) assumption that allows us to simplify this formula, however.

First, by Baye's Theorem,
$$P(Y|X) = \frac{P(Y)*P(X|Y)}{P(X)}.$$
Now the difficult thing to calculate here is $P(X|Y)$; however, if we assume that all X are conditionally independent given Y, we get
$$\frac{P(Y)*P(X|Y)}{P(X)} = \frac{P(Y)*\prod_{i=1}^{d}P(X_i|Y)}{P(X)}.$$
Now, excluding the constant denominator, we get the objective function that the Bayesian classifier seeks to maximize:
$$P(Y)*\prod_{i=1}^{d}P(X_i|Y).$$
>[!faq] Implications of conditional independence
> - Conditional independence is NOT reciprocal; that is, $P(X,Y|Z) = P(X|Z)*P(Y|Z)$ does *not* imply $P(X,Z|Y) = P(X|Y)*P(Z|Y)$
> - Conditional independence does NOT imply unconditional independence
> - Does NOT imply that there is no interaction information between independent variables and the target variable
> 
> ![[Pasted image 20250212154905.png|500]]


