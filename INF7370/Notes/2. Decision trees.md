
# ðŸ¤” What is a [[2. Decision trees slides.pdf|decision tree]]?
---
A decision tree is an algorithm that categorizes data by its attributes to predict the target variable; in other words, a decision tree groups people/data points in boxes based on their characteristics to judge them; ==scientific stereotyping==, if you will. For example, if we are using a decision tree to predict whether a borrower will default on their loan, the decision tree will first profile the borrower based on relevant attributes (e.g. marital status, employment status, home ownership, etc.) to make a best guess based on how often other people with this same profile have defaulted. Here is a simple example of what such a decision tree might look like:

![[Pasted image 20250129223301.png|500]]

But how does a decision tree learn which attributes are relevant?





# ðŸŒ³ Growing a decision tree: the basics
---
A decision tree starts off as a root; at this stage, the tree is perfectly indiscriminate, that is, every data point looks the same to it.

**The first step in branching out** a tree involves choosing a variable based on which to discriminate. In the best case scenario, this variable would be a perfect indicator of our outcome variable; every celibate always defaults on their loan, and every married person always pays it back. This would result in an extremely clean, or *pure*, division, and this purity is precisely what growing trees are after â€” indeed, ==the tree will decide which variable to discriminate by purely based on the purity of the resulting split==.
#### Measuring impurity
Two common measures of impurity are [[Key concepts#^entropy|entropy]] (algorithms: ID3 or C4.5) and the [[Key concepts#^gini|Gini coefficient]] (algorithm: CART).

![[Key concepts#^entropy]]
![[Key concepts#^Gini]]
Whichever metric is used, a tree will always branch out in the direction that minimizes the tree's impurity, which is the weighted sum of the impurity of the tree's leaves ([[Pasted image 20250203171830.png|here is the formula using Gini from CART]], and an [[Pasted image 20250130005758.png|example calculation using entropy]]).
#### What about numerical variables?

Numerical variables are problematic: if every value of a numerical variable constitutes its own category, this can easily lead to overfitting. One can easily imagine a dataset where one continuous feature never takes the same value twice. ==A common solution is to categorize the numerical variable by intervals.== ^numvar
###### How to choose the intervals
Start by making just two intervals out of the numerical variable (one â€œhighâ€ and one â€œlowâ€), and choose the threshold that separates these intervals to be that which minimizes impurity ([[Pasted image 20250203025627.png|example]] using Scikit-Learn's [[Pasted image 20250203165419.png|CART algorithm loss function, or weighted Gini]]).

More subcategories can then be recursively created by subdividing the existing intervals into two smaller intervals, similarly to how a binary tree would grow.
# âœ‚ï¸ Refining decision trees
---
Branching out almost always leads to *some* information gain; however, letting a tree grow indefinitely can easily lead to overfitting. Here are some common methods used to counter overfitting.
#### Parameter limits
The simplest way to prevent overfitting is to set hard limits on how far the tree can grow. Here is how this can be done, with corresponding parameters from SKLearn's DecisionTreeClassifier class:
- **min_samples_split**: imposing a minimum splittable node size
- **min_samples_leaf** or **min_weight_fraction_leaf**: imposing a minimum leaf node size
- **max_lead_nodes**: putting a ceiling on the number of leaves the tree can produce
- **max_depth**: setting a limit on the treeâ€™s depth
- **max_features**: max number of features checked at each split
#### Trimming
The most computationally extravagant way to mitigate overfitting is to grow the decision tree all the way out, test it on the development set, trim off the least valuable node (as defined below), and test it again on the development set; again trim off the least valuable of remaining nodes, and test again, and so on until only the root is left. Keep the tree that performed best on the development set. This algorithm is summarized on [[2. Decision trees slides.pdf#page=91|this slide]].
>[!info] A nodeâ€™s value
>A nodeâ€™s value is quantified by:
>$$\dfrac{\#\ errors\ corrected\ by\ node}{\#\ leaves\ under\ node}$$
>Which is a simplifcation of the real formula:
> ![[Pasted image 20250203184003.png|500]]
>



# ðŸ“ˆRegression trees
---
Decision trees can also be used to predict continuous variables. Every leaf node's prediction is the mean value of all training data in the leaf node, and the loss function is MSE instead of impurity.

CART algorithm will try out different thresholds and choose the one that minimizes MSE.

# ðŸ“‹Pros and Cons
---
**Pros:**
- Readability
- Easy parameters
- Widely available

**Cons:**
- Non-incremental (must retrain whole tree to integrate new data)
- Splits are perpendicular to axes (sensitive to [[Pasted image 20250203185511.png|data rotation]])