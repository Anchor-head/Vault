# How to feed a training neural net
![[8. ANN slides.pdf#page=24]]


# The Perceptron

The Perceptron is a neural network with no hidden layers. The output (or, each output feature, if there are multiple) is the sum of all weighted inputs (including [[#^bias|bias]]); often, an **activation function** is applied to this sum, like the sigmoid function.

![[Pasted image 20250318201900.png]]
Here, $\sigma_{ik}$ represents the sum of all weighted inputs of node k at training iteration i.

### It turns out that the perceptron is a linear classifier.

![[8. ANN slides.pdf#page=50]]
![[8. ANN slides.pdf#page=52]]

# Multilayer Neural Network
![[8. ANN slides.pdf#page=61]]


# How to update weights during training (backpropagation)

Just compute the derivative of the loss function for each output feature with respect to each weight (the gradient) and update the weights in the opposite direction (to reduce loss), proportionally to their impact on the loss.
![[8. ANN slides.pdf#page=67]]

This derivative will depend on the activation function. For a sigmoid activation function, the math goes like this: 
![[8. ANN slides.pdf#page=69]]
![[8. ANN slides.pdf#page=71]]

# Annexe

 >[!note] Bias can be conceptualized as an extra constant input feature, controlled by its weight. ^bias