# Different Types of Gradient Descent

- **Stochastic gradient descent** is when you feed the neural network one data at a time and backpropagate for every single data point.
	- **Time**: long
	- **Memory requirement**: minimal
	- **Stability**: bad

- **Gradient descent** is when you feed the neural network the whole training dataset at once, and only backpropagate once.
	- **Time**: short
	- **Memory requirement**: infeasible
	- **Stability**: great

- **Mini-batch gradient descent** (mini-lot) is when you split the training data into mini-batches and do backpropagation for every mini-batch.
	- **Time**: medium
	- **Memory requirement**: adjustable (batch size)
	- **Stability**: good


![[9. Deep learning slides.pdf#page=9]]

### Mini-batch gradient descent hyperparameters

- 