# Different Types of Gradient Descent

- **Stochastic gradient descent** is when you feed the neural network one data at a time and backpropagate for every single data point.
	- **Time**: long
	- **Memory requirement**: minimal
	- **Stability/convergence**: bad

- **Gradient descent** is when you feed the neural network the whole training dataset at once, and only backpropagate once.
	- **Time**: short
	- **Memory requirement**: infeasible
	- **Stability/convergence**: great

- **Mini-batch gradient descent** (mini-lot) is when you split the training data into mini-batches and do backpropagation for every mini-batch.
	- **Time**: medium
	- **Memory requirement**: adjustable (batch size)
	- **Stability/convergence**: good


![[9. Deep learning slides.pdf#page=9]]

# Mini-batch gradient descent hyperparameters

###### **Batch size (16, 32, 64, 128)**
Start with 32. If your computer can't handle the memory, go down.
###### **Iterations**
Number of batches in an epoch $(\dfrac{training\space set\space size}{batch\space size})$
###### **Epochs**
Number of times to feed the whole dataset.
###### **Cost function**
Mean squared error (erreur quadratique) $$\dfrac{1}{n}\sum_{i=1}^{n}{(y_i-\hat{y}_i)^2}$$Cross-entropy (entropie croisée) $$-\dfrac{1}{n}\sum_{i=1}^{n}{[y_i*ln\hat{y}_i+(1-y_i)*ln(1-\hat{y}_i)]}$$
###### **Step size/learning rate**
![[9. Deep learning slides.pdf#page=19]]






# Exit node activation function

- Sigmoid if binary problem $$\dfrac{e^x}{1+e^x}$$
- Softmax otherwise $$\dfrac{e^{x_j}}{\sum_i{e^{x_i}}}$$
# *How to mitigate vanishing, exploding, and unstable gradient

### *Hidden node activation function

![[9. Deep learning slides.pdf#page=31]]
Sigmoid function is prone to saturation.

#### *Tanh

*La fonction tanh accélère la convergence comparé à la fonction sigmoïde.
![[9. Deep learning slides.pdf#page=32]]

#### ReLU family
###### ReLU
ReLU is by far the most used activation function. But neurons die when they reach negative output (pre-activation) because gradient is 0.
![[9. Deep learning slides.pdf#page=34]]

###### Leaky ReLU family
This seeks to give neurons life that would otherwise be dead under ReLU.
![[9. Deep learning slides.pdf#page=36]]
![[9. Deep learning slides.pdf#page=39]]
###### ELU family
Smoothed version of Leaky ReLU, with a lower limit at -1.
![[9. Deep learning slides.pdf#page=41]]
![[9. Deep learning slides.pdf#page=42]]
![[9. Deep learning slides.pdf#page=43]]

![[9. Deep learning slides.pdf#page=46]]

### Weight initialization
#### Glorot initialization (Keras default)
Significantly accelerates training, major contributor to success of deep learning.
![[9. Deep learning slides.pdf#page=49]]
![[9. Deep learning slides.pdf#page=50]]
#### LeCun initialization
Same as Glorot initialization, but replace $n_{moyen}$ by $n_{in}$. These two methods are equivalent if $n_{out}=n_{in}$.

#### He initialization
Same as LeCun, but double the variance: $\sigma=\sqrt{\dfrac{2}{n_{in}}}$.
For uniform distribution, $r=\sqrt{3\sigma^2}$ (same formula as Glorot uniform).


### Batch normalization (normalisation par lots)

Mechanism: introduce a BN layer before or after the activation function of each hidden layer that normalizes the outputs before scaling and shifting them to an optimal scale and shift. The *scale* and *shift* parameters are learnable for each node via classic backpropagation.
![[9. Deep learning slides.pdf#page=58]]

Purpose:
1. Mitigate exploding and vanishing gradient (activation function and initialization techniques only mitigate these problems in early training).
2. Prevent top layers from being affected by changes in scale of earlier layers.


# Deep learning optimizers
 SGD (Stochastic Gradient Descent) is very slow for very big deep neural networks. There are other optimizers than SGD that are faster.
### Momentum
Accumulate momentum in gradient descent, just like a ball rolling down a slope. This allows the algorithm to skip local minima (but also the global minimum).

![[9. Deep learning slides.pdf#page=68]]
Default: $\beta=0.9$
### Nesterov Accelerated Gradient (NAG)
Like Momentum, but instead of moving by "gradient at current location + momentum", move by "momentum + gradient at the location after moving by the momentum".

![[9. Deep learning slides.pdf#page=71]]

### AdaGrad (Adaptive Gradient)

Divide the step size by the square root of the sum of squared past gradients.

![[9. Deep learning slides.pdf#page=74]]
Not a very good method, tends to stop prematurely. This is only a gateway to understanding future methods.

### RMSProp (Root Mean Square Propagation)

Same as AdaGrad but only store recent gradients by using an exponential moving average (of the squared gradients) with smoothing parameter $\rho=0.9$ (taux de décroissance).

![[9. Deep learning slides.pdf#page=78]]

### Adam (Adaptive Movement Estimation)

![[9. Deep learning slides.pdf#page=80]]
![[9. Deep learning slides.pdf#page=81]]
Notice the similarity to a Newton step.

Initial hyperparameter values:
$\beta_1=0.9$
$\beta_2=0.999$
$\alpha=0.001$ (matters less since step size is adaptive).

Steps 3 and 4 are used to dynamize $m_{ij}$ and $s_{ij}$ in the beginning of training, since they are initialized at 0. These steps do virtually nothing after a few dozen iterations.

### Performance comprison
![[9. Deep learning slides.pdf#page=84]]


# Preventing overfitting
### Early stopping
![[9. Deep learning slides.pdf#page=88]]
Evaluate model on validation set every x steps (epochs?). Stop training if performance on validation set does not improve after y steps.

### $l_1$ and $l_2$ regularization

