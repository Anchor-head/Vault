# Different Types of Gradient Descent

- **Stochastic gradient descent** is when you feed the neural network one data at a time and backpropagate for every single data point.
	- **Time**: long
	- **Memory requirement**: minimal
	- **Stability/convergence**: bad

- **Gradient descent** is when you feed the neural network the whole training dataset at once, and only backpropagate once.
	- **Time**: short
	- **Memory requirement**: infeasible
	- **Stability/convergence**: great

- **Mini-batch gradient descent** (mini-lot) is when you split the training data into mini-batches and do backpropagation for every mini-batch.
	- **Time**: medium
	- **Memory requirement**: adjustable (batch size)
	- **Stability/convergence**: good


![[9. Deep learning slides.pdf#page=9]]

# Mini-batch gradient descent hyperparameters

###### **Batch size (16, 32, 64, 128)**
Start with 32. If your computer can't handle the memory, go down.
###### **Iterations**
Number of batches in an epoch $(\dfrac{training\space set\space size}{batch\space size})$
###### **Epochs**
Number of times to feed the whole dataset.
###### **Cost function**
Mean squared error (erreur quadratique) $$\dfrac{1}{n}\sum_{i=1}^{n}{(y_i-\hat{y}_i)^2}$$Cross-entropy (entropie crois√©e) $$-\dfrac{1}{n}\sum_{i=1}^{n}{[y_i*ln\hat{y}_i+(1-y_i)*ln(1-\hat{y}_i)]}$$
###### **Step size/learning rate**
![[9. Deep learning slides.pdf#page=19]]






# Exit node activation function

- Sigmoid if binary problem $$\dfrac{e^x}{1+e^x}$$
- Softmax otherwise $$\dfrac{e^{x_j}}{\sum_i{e^{x_i}}}$$
# How to mitigate vanishing, exploding, and unstable gradient

### Hidden node activation function

![[9. Deep learning slides.pdf#page=31]]

### Weight initialization

### Batch normalization (normalisation par lots)