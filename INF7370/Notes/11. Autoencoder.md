# ðŸ¤” What is an [[11. Autoencoder slides.pdf|autoencoder]]?

An autoencoder is an unsupervised/self-supervised neural network that learns dense (latent) representations (A.K.A. encodings) of input. PCA++.

Often used in non-supervised pretraining for deep NNs.

![[11. Autoencoder slides.pdf#page=4]]

The architecture is most often symmetrical, even for deep autoencoders containing more than the 3 essential layers.
# How do you train it?

The autoencoder's training objective is to faithfully reproduce the input from the narrow encoding using a decoder.

The outer layers are trained first.
![[11. Autoencoder slides.pdf#page=11]]

The decoder's weights are tied to the encoder's weights, being their transpose.
![[11. Autoencoder slides.pdf#page=13]]

![[11. Autoencoder slides.pdf#page=14]]

This halves the number of parameters to be learned, and also serves regularization. See more on the reason behind using the transpose.^[https://stats.stackexchange.com/a/533427/410763]
### Supervised training

Once the auto-encoder has been trained, we can use the encodings as inputs to an MLP or an NN.
![[11. Autoencoder slides.pdf#page=15]]
 The encoder itself can be fine-tuned at this stage.


### Denoising autoencoders
![[11. Autoencoder slides.pdf#page=19]]
![[11. Autoencoder slides.pdf#page=20]]
![[11. Autoencoder slides.pdf#page=23]]

# Convolutional auto-encoders
![[11. Autoencoder slides.pdf#page=26]]
![[11. Autoencoder slides.pdf#page=27]]



