# 🤔 What are [[3. Apprentissage par ensemble.pdf|ensemble methods]]?
---
Instead of just using one predictive model, why not use a bunch of classifiers and return the majority classification? That's what ensemble methods are.

As it turns out, this can improve the error rate a lot — to the extent that the different classifiers are independent and their error is better than random (a committee of clones is quite useless, and a committee of blinds is also useless).

![[Pasted image 20250206175020.png]][[Pasted image 20250206175542.png|Here is the formula for the error rate of an ensemble]] of $N$ independent classifiers with error rate $ε$.

# 👜 Bagging
---
![[Pasted image 20250207111747.png]]
Bagging (short for "bootstrap aggregation") involves creating multiple training sets out of the original training via bootstrapping (bootstrapping is resampling with replacement). The training sets created this way are called "bootstraps," and are generally the same size as the original training set.
![[Pasted image 20250207111922.png]]


# 🚀 Boosting
---
![[Pasted image 20250207112045.png]]
Boosting involves sequentially training a classifier on different non-random splits of the training data; the point is to make each split overrepresent the misclassifications made by the classifier trained on the previous split, to continually “train them on their weakness” and create adversarial classifiers that cover each other’s blind spots.
#### Elementary boosting
Sampling is done without replacement.

![[Pasted image 20250207112404.png]]![[Pasted image 20250207135110.png]]
Schapire proved that splitting the training data in this way does indeed improve performance.
#### Probabilistic boosting
>[!warning]  Different sampling
>This time, sampling is done ***with*** replacement, and samples are the size of the original training data.

![[IMG_3691.jpeg]]
The idea is to increase the weight of misclassified data every sample. This is where **AdaBoost** (adaptive boosting) comes in.

![[IMG_3694.jpeg]]![[IMG_3692.jpeg]]![[IMG_3693.jpeg]]

Note that the weights $w_{i}$ are relevant both to the selection of the training sample for classifier $h_i$ and to the error of $h_i$ which determines the update of the weights $w_{i+1}$.

# 🌲 Random Forest
Random forest is a simple ensemble method that involves taking the majority output of a set of random trees.

![[Pasted image 20250210093602.png]]
This method is useful for very high-dimensional data; it also helps determine which features are most important.

The metric for the importance of a feature is Mean Decrease in Impurity, which Scikit-Learn's RandomForestClassifier automatically calculates and stores in the variable *feature\_importances\_*. 