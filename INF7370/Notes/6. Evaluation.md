# 🏁 Cross-validation

Cross-validation involves splitting the training set into k even chunks, reserving one chunk for testing; k different models are trained each with different chunks reserved for testing.

![[6. Evaluation slides.pdf#page=7]]

The error of the model is simply the average of every model's error on its respective training set.

>[!note] Leave-one-out
>For small training datasets, an extreme version of cross-validation can be implemented where k=n.
>![[Pasted image 20250305195841.png|300]]

# Confusion matrix

A confusion matrix is an NxN matrix which is basically a table that logs all the different types of classification errors made by a model. Columns represent the training data's true labels, while rows represent the model's prediction; element $n_{ij}$ is the number of instances of class j classified as i by the model. Therefore, the matrix' diagonal represents the number of correctly classified instances.

Classification error is the proportion of misclassifications: $$\frac{\sum_i{\sum_{j\neq i}{n_{ij}}}}{N}$$
#### F1 score

In a binary classification problem, the confusion matrix is 2x2. One class is positive, one is negative, and now more evaluation metrics like precision, recall, and F1 become available.

F1=$\dfrac{2*precision*recall}{precision+recall}$



