# üèÅ Cross-validation

Cross-validation involves splitting the training set into k even chunks, reserving one chunk for testing; k different models are trained each with different chunks reserved for testing.

![[6. Evaluation slides.pdf#page=7]]

The error of the model is simply the average of every model's error on its respective training set.

>[!note] Leave-one-out
>For small training datasets, an extreme version of cross-validation can be implemented where k=n.
>![[Pasted image 20250305195841.png|300]]

# Confusion matrix

A confusion matrix is an NxN matrix which is basically a table that logs all the different types of classification errors made by a model. Columns represent the training data's true labels, while rows represent the model's prediction; element $n_{ij}$ is the number of instances of class j classified as i by the model. Therefore, the matrix' diagonal represents the number of correctly classified instances.

Classification error is the proportion of misclassifications: $$\frac{\sum_i{\sum_{j\neq i}{n_{ij}}}}{N}$$
#### F1 score

In a binary classification problem (which any classification problem can be analyzed as, given one target class), one class is positive, one is negative, and now more evaluation metrics like precision, recall, and F1 become available.
![[6. Evaluation slides.pdf#page=20]]

F1 score favours models with similar precision and recall.

# ROC curve

False positives and false negatives are at odds; mitigating one often involves exacerbating the other. The ROC (Receiver Operating Characteristic) curve (courbe d'efficacit√© du r√©cepteur) plots false positive rate, FP/(FP+TN), on the x-axis to true positive rate (or recall) in the y-axis.
![[6. Evaluation slides.pdf#page=34]]
This curve is useful for determining the threshold in a classifier that output scores or probabilities of belonging to a class, as opposed to pure classes like decision trees.
![[6. Evaluation slides.pdf#page=40]]

